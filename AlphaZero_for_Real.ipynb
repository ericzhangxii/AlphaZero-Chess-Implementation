{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVK9yUtsi8R+eLDMbLSVWU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ericzhangxii/AlphaZero-Chess-Implementation/blob/main/AlphaZero_for_Real.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup"
      ],
      "metadata": {
        "id": "fySgtbj0P7vY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install chess\n",
        "\n"
      ],
      "metadata": {
        "id": "nss7p1byvkix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import tensorflow as tf\n",
        "import copy\n",
        "import math\n",
        "import numpy\n",
        "from typing import List\n",
        "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Input, BatchNormalization, Add, Activation\n",
        "from tensorflow.keras.models import Model"
      ],
      "metadata": {
        "id": "au9nCsduY-Fg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Unmodified Helpers"
      ],
      "metadata": {
        "id": "9tJP4gAXP2dp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "##########################\n",
        "####### Helpers ##########\n",
        "\n",
        "\n",
        "class AlphaZeroConfig(object):\n",
        "\n",
        "  def __init__(self):\n",
        "    ### Self-Play\n",
        "    self.num_actors = 5000\n",
        "\n",
        "    self.num_sampling_moves = 30\n",
        "    self.max_moves = 512  # for chess and shogi, 722 for Go.\n",
        "    self.num_simulations = 800\n",
        "\n",
        "    # Root prior exploration noise.\n",
        "    self.root_dirichlet_alpha = 0.3  # for chess, 0.03 for Go and 0.15 for shogi.\n",
        "    self.root_exploration_fraction = 0.25\n",
        "\n",
        "    # UCB formula\n",
        "    self.pb_c_base = 19652\n",
        "    self.pb_c_init = 1.25\n",
        "\n",
        "    ### Training\n",
        "    self.training_steps = int(700e3)\n",
        "    self.checkpoint_interval = int(1e3)\n",
        "    self.window_size = int(1e6)\n",
        "    self.batch_size = 4096\n",
        "\n",
        "    self.weight_decay = 1e-4\n",
        "    self.momentum = 0.9\n",
        "    # Schedule for chess and shogi, Go starts at 2e-2 immediately.\n",
        "    self.learning_rate_schedule = {\n",
        "        0: 2e-1,\n",
        "        100e3: 2e-2,\n",
        "        300e3: 2e-3,\n",
        "        500e3: 2e-4\n",
        "    }\n",
        "\n",
        "\n",
        "class Node(object):\n",
        "\n",
        "  def __init__(self, prior: float):\n",
        "    self.visit_count = 0\n",
        "    self.to_play = -1\n",
        "    self.prior = prior\n",
        "    self.value_sum = 0\n",
        "    self.children = {}\n",
        "\n",
        "  def expanded(self):\n",
        "    return len(self.children) > 0\n",
        "\n",
        "  def value(self):\n",
        "    if self.visit_count == 0:\n",
        "      return 0\n",
        "    return self.value_sum / self.visit_count\n",
        "\n",
        "class Game(object):\n",
        "  pass\n",
        "\n",
        "class ReplayBuffer(object):\n",
        "\n",
        "  def __init__(self, config: AlphaZeroConfig):\n",
        "    self.window_size = config.window_size\n",
        "    self.batch_size = config.batch_size\n",
        "    self.buffer = []\n",
        "\n",
        "  def save_game(self, game):\n",
        "    if len(self.buffer) > self.window_size:\n",
        "      self.buffer.pop(0)\n",
        "    self.buffer.append(game)\n",
        "\n",
        "  def sample_batch(self):\n",
        "    # Sample uniformly across positions.\n",
        "    move_sum = float(sum(len(g.history) for g in self.buffer))\n",
        "    games = numpy.random.choice(\n",
        "        self.buffer,\n",
        "        size=self.batch_size,\n",
        "        p=[len(g.history) / move_sum for g in self.buffer])\n",
        "    game_pos = [(g, numpy.random.randint(len(g.history))) for g in games]\n",
        "    return [(g.make_image(i), g.make_target(i)) for (g, i) in game_pos] # returns image, [target value, target policy] for a random game position\n",
        "\n",
        "class Network(object):\n",
        "  pass\n",
        "  \n",
        "class SharedStorage(object):\n",
        "  def __init__(self):\n",
        "    self._networks = {}\n",
        "\n",
        "  def latest_network(self) -> Network:\n",
        "    if self._networks:\n",
        "      return self._networks[max(self._networks.iterkeys())]\n",
        "    else:\n",
        "      return make_uniform_network()  # policy -> uniform, value -> 0.5\n",
        "\n",
        "  def save_network(self, step: int, network: Network):\n",
        "    self._networks[step] = network\n",
        "\n",
        "\n",
        "##### End Helpers ########\n",
        "##########################"
      ],
      "metadata": {
        "id": "nLFMJmsWcGQF"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Game Class"
      ],
      "metadata": {
        "id": "NA_w_DAYQKRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Game(object):\n",
        "\n",
        "  def __init__(self, history=None):\n",
        "    self.history = history or []\n",
        "    self.child_visits = []\n",
        "    self.num_actions = 4672  # action space size for chess; 11259 for shogi, 362 for Go\n",
        "    self.board = chess.Board()\n",
        "\n",
        "  def terminal(self):\n",
        "    # Game specific termination rules.\n",
        "    return self.board.is_game_over()\n",
        "\n",
        "  def terminal_value(self, to_play): #to_play here represents the player in the turn evaluated\n",
        "    # Game specific value.\n",
        "    if(self.terminal()):\n",
        "      if(self.board.is_checkmate()):\n",
        "        if(to_play == self.to_play): #If the player evaluated is the same as the player in checkmate, return -1\n",
        "          return -1\n",
        "        else:\n",
        "          return 1\n",
        "      else:\n",
        "        return 0\n",
        "    else:\n",
        "      return None\n",
        "\n",
        "  def legal_actions(self):\n",
        "    # Game specific calculation of legal actions.\n",
        "    return list(self.board.legal_moves)\n",
        "\n",
        "  def clone(self):\n",
        "    return copy.deepcopy(self)\n",
        "\n",
        "  def apply(self, action):\n",
        "    scratch_board = copy.deepcopy(self.board)\n",
        "    scratch_board.push(action)\n",
        "    self.history.append(scratch_board)\n",
        "    self.board = copy.deepcopy(scratch_board)\n",
        "\n",
        "\n",
        "  def store_search_statistics(self, root):\n",
        "    sum_visits = sum(child.visit_count for child in root.children.itervalues())\n",
        "    self.child_visits.append([\n",
        "        root.children[a].visit_count / sum_visits if a in root.children else 0\n",
        "        for a in range(self.num_actions)\n",
        "    ])\n",
        "\n",
        "  def make_image(self, state_index: int):\n",
        "    # Game specific feature planes. Used in two contexts: for evaluating during MCTS and during training for a specific time step. All in Network.inference()\n",
        "    # The list it's looking for is evidently \"8 time steps\", counting back from the state index. \n",
        "    if(len(self.history)==0):\n",
        "      return board_to_planes(self.board)\n",
        "    board = self.history[state_index]\n",
        "    return board_to_planes(board)\n",
        "\n",
        "  def make_target(self, state_index: int):\n",
        "    #returns target weight and target policy, the policy is apparently based on child visits?\n",
        "    return (self.terminal_value(state_index % 2),\n",
        "            self.child_visits[state_index])\n",
        "\n",
        "  def to_play(self):\n",
        "    return len(self.history) % 2 #white if 0, black if 1"
      ],
      "metadata": {
        "id": "hP01YgyJQN_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Helpers"
      ],
      "metadata": {
        "id": "eqgjLVouQi2f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import chess\n",
        "import numpy as np\n",
        "\n",
        "def board_to_planes(board):\n",
        "    \"\"\"\n",
        "    Convert a chess.Board() object into a set of planes representing the board state.\n",
        "    \"\"\"\n",
        "    # Create a tensor with shape (8, 8, 12) to store the piece information\n",
        "    planes = np.zeros((8, 8, 12), dtype=np.uint8)\n",
        "\n",
        "    # Iterate through the board and set the corresponding plane values\n",
        "    for i in range(64):\n",
        "        piece = board.piece_at(i)\n",
        "        if piece:\n",
        "            color = int(piece.color)\n",
        "            piece_type = piece.piece_type - 1\n",
        "            row, col = divmod(i, 8)\n",
        "            planes[row, col, color * 6 + piece_type] = 1\n",
        "\n",
        "    return planes\n",
        "\n",
        "def history_to_tensor(history, state_index=-1):\n",
        "    \"\"\"\n",
        "    Convert a history of chess board positions into a tensor.\n",
        "    \"\"\"\n",
        "    T = 8\n",
        "    M = 12\n",
        "    L = 7\n",
        "    N = 8\n",
        "\n",
        "    # If state_index is -1, use the last 8 board positions\n",
        "    if state_index == -1:\n",
        "        state_index = max(0, len(history) - T)\n",
        "\n",
        "    # Initialize the tensor with shape (8, 8, MT + L)\n",
        "    tensor = np.zeros((N, N, M * T + L), dtype=np.uint8)\n",
        "\n",
        "    # Fill in the tensor with the board positions\n",
        "    for t in range(T):\n",
        "        if t < len(history) - state_index:\n",
        "            board = history[state_index + t]\n",
        "            tensor[:, :, M * t:M * (t + 1)] = board_to_planes(board)\n",
        "\n",
        "    # Fill in the constant-valued input planes\n",
        "    last_board = history[-1]\n",
        "    tensor[:, :, -L] = int(last_board.turn)\n",
        "    tensor[:, :, -L + 1] = last_board.fullmove_number\n",
        "    tensor[:, :, -L + 2] = int(last_board.has_kingside_castling_rights(chess.WHITE))\n",
        "    tensor[:, :, -L + 3] = int(last_board.has_kingside_castling_rights(chess.BLACK))\n",
        "    tensor[:, :, -L + 4] = int(last_board.has_queenside_castling_rights(chess.WHITE))\n",
        "    tensor[:, :, -L + 5] = int(last_board.has_queenside_castling_rights(chess.BLACK))\n",
        "    tensor[:, :, -L + 6] = last_board.halfmove_clock\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "c3OxKMKp2tk5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modified Network\n"
      ],
      "metadata": {
        "id": "7omhILaLQcbk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class chessNetwork(Network):\n",
        "    def __init__(self):\n",
        "        self.model = self.build_chess_model()\n",
        "        self.model.compile(optimizer='adam', loss=['mean_squared_error', 'categorical_crossentropy'])\n",
        "\n",
        "    def build_chess_model(self):\n",
        "        input_shape = (8, 8, 12)\n",
        "        n_actions = 4672\n",
        "\n",
        "        input_board = Input(shape=input_shape)\n",
        "\n",
        "        def residual_block(x):\n",
        "            res = x\n",
        "            x = Conv2D(256, 3, padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Activation('relu')(x)\n",
        "            x = Conv2D(256, 3, padding='same')(x)\n",
        "            x = BatchNormalization()(x)\n",
        "            x = Add()([x, res])\n",
        "            x = Activation('relu')(x)\n",
        "            return x\n",
        "\n",
        "        x = Conv2D(256, 3, padding='same')(input_board)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "        for _ in range(5):\n",
        "            x = residual_block(x)\n",
        "\n",
        "        value = Conv2D(1, 1, padding='same')(x)\n",
        "        value = BatchNormalization()(value)\n",
        "        value = Activation('relu')(value)\n",
        "        value = Flatten()(value)\n",
        "        value = Dense(256, activation='relu')(value)\n",
        "        value = Dense(1, activation='tanh')(value)\n",
        "\n",
        "        policy = Conv2D(73, 1, padding='same')(x)\n",
        "        policy = BatchNormalization()(policy)\n",
        "        policy = Activation('relu')(policy)\n",
        "        policy = Flatten()(policy)\n",
        "        policy = Dense(n_actions, activation='softmax')(policy)\n",
        "\n",
        "        model = Model(inputs=input_board, outputs=[value, policy])\n",
        "\n",
        "        return model\n",
        "\n",
        "    def inference(self, image):\n",
        "        input_data = np.expand_dims(image, axis=0)\n",
        "        value, policy = self.model.predict(input_data)\n",
        "        policy /= policy.sum()\n",
        "        move_prob_dict = {}\n",
        "        return value[0], policy[0]\n",
        "\n",
        "    def get_weights(self):\n",
        "        return self.model.get_weights()\n"
      ],
      "metadata": {
        "id": "KnY2GFTQQelV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self Play and Training"
      ],
      "metadata": {
        "id": "RRyHMK1lRwuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "# AlphaZero training is split into two independent parts: Network training and\n",
        "# self-play data generation.\n",
        "# These two parts only communicate by transferring the latest network checkpoint\n",
        "# from the training to the self-play, and the finished games from the self-play\n",
        "# to the training.\n",
        "def alphazero(config: AlphaZeroConfig):\n",
        "  storage = SharedStorage()\n",
        "  replay_buffer = ReplayBuffer(config)\n",
        "\n",
        "  for i in range(config.num_actors):\n",
        "    launch_job(run_selfplay, config, storage, replay_buffer)\n",
        "\n",
        "  train_network(config, storage, replay_buffer)\n",
        "\n",
        "  return storage.latest_network()\n",
        "\n",
        "\n",
        "##################################\n",
        "####### Part 1: Self-Play ########\n",
        "\n",
        "\n",
        "# Each self-play job is independent of all others; it takes the latest network\n",
        "# snapshot, produces a game and makes it available to the training job by\n",
        "# writing it to a shared replay buffer.\n",
        "def run_selfplay(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                 replay_buffer: ReplayBuffer):\n",
        "  while True:\n",
        "    network = storage.latest_network()\n",
        "    game = play_game(config, network)\n",
        "    replay_buffer.save_game(game)\n",
        "\n",
        "\n",
        "# Each game is produced by starting at the initial board position, then\n",
        "# repeatedly executing a Monte Carlo Tree Search to generate moves until the end\n",
        "# of the game is reached.\n",
        "def play_game(config: AlphaZeroConfig, network: Network):\n",
        "  game = Game()\n",
        "  while not game.terminal() and len(game.history) < config.max_moves:\n",
        "    action, root = run_mcts(config, game, network)\n",
        "    game.apply(action)\n",
        "    game.store_search_statistics(root)\n",
        "  return game\n",
        "\n",
        "\n",
        "# Core Monte Carlo Tree Search algorithm.\n",
        "# To decide on an action, we run N simulations, always starting at the root of\n",
        "# the search tree and traversing the tree according to the UCB formula until we\n",
        "# reach a leaf node.\n",
        "def run_mcts(config: AlphaZeroConfig, game: Game, network: Network):\n",
        "  root = Node(0)\n",
        "  evaluate(root, game, network)\n",
        "  add_exploration_noise(config, root)\n",
        "\n",
        "  for _ in range(config.num_simulations):\n",
        "    node = root\n",
        "    scratch_game = game.clone()\n",
        "    search_path = [node]\n",
        "\n",
        "    while node.expanded():\n",
        "      action, node = select_child(config, node)\n",
        "      scratch_game.apply(action)\n",
        "      search_path.append(node)\n",
        "\n",
        "    value = evaluate(node, scratch_game, network)\n",
        "    backpropagate(search_path, value, scratch_game.to_play())\n",
        "  return select_action(config, game, root), root\n",
        "\n",
        "\n",
        "def select_action(config: AlphaZeroConfig, game: Game, root: Node):\n",
        "  visit_counts = [(child.visit_count, action)\n",
        "                  for action, child in root.children.iteritems()]\n",
        "  if len(game.history) < config.num_sampling_moves:\n",
        "    _, action = softmax_sample(visit_counts)\n",
        "  else:\n",
        "    _, action = max(visit_counts)\n",
        "  return action\n",
        "\n",
        "\n",
        "# Select the child with the highest UCB score.\n",
        "def select_child(config: AlphaZeroConfig, node: Node):\n",
        "  _, action, child = max((ucb_score(config, node, child), action, child)\n",
        "                         for action, child in node.children.iteritems())\n",
        "  return action, child\n",
        "\n",
        "\n",
        "# The score for a node is based on its value, plus an exploration bonus based on\n",
        "# the prior.\n",
        "def ucb_score(config: AlphaZeroConfig, parent: Node, child: Node):\n",
        "  pb_c = math.log((parent.visit_count + config.pb_c_base + 1) /\n",
        "                  config.pb_c_base) + config.pb_c_init\n",
        "  pb_c *= math.sqrt(parent.visit_count) / (child.visit_count + 1)\n",
        "\n",
        "  prior_score = pb_c * child.prior\n",
        "  value_score = child.value()\n",
        "  return prior_score + value_score\n",
        "\n",
        "\n",
        "# We use the neural network to obtain a value and policy prediction.\n",
        "def evaluate(node: Node, game: Game, network: Network):\n",
        "  value, policy_logits = network.inference(game.make_image(-1))\n",
        "\n",
        "  # Expand the node.\n",
        "  node.to_play = game.to_play()\n",
        "  policy = {a: math.exp(policy_logits[a]) for a in game.legal_actions()}\n",
        "  policy_sum = sum(policy.itervalues())\n",
        "  for action, p in policy.iteritems():\n",
        "    node.children[action] = Node(p / policy_sum)\n",
        "  return value\n",
        "\n",
        "\n",
        "# At the end of a simulation, we propagate the evaluation all the way up the\n",
        "# tree to the root.\n",
        "def backpropagate(search_path: List[Node], value: float, to_play):\n",
        "  for node in search_path:\n",
        "    node.value_sum += value if node.to_play == to_play else (1 - value)\n",
        "    node.visit_count += 1\n",
        "\n",
        "\n",
        "# At the start of each search, we add dirichlet noise to the prior of the root\n",
        "# to encourage the search to explore new actions.\n",
        "def add_exploration_noise(config: AlphaZeroConfig, node: Node):\n",
        "  actions = node.children.keys()\n",
        "  noise = numpy.random.gamma(config.root_dirichlet_alpha, 1, len(actions))\n",
        "  frac = config.root_exploration_fraction\n",
        "  for a, n in zip(actions, noise):\n",
        "    node.children[a].prior = node.children[a].prior * (1 - frac) + n * frac\n",
        "\n",
        "\n",
        "######### End Self-Play ##########\n",
        "##################################\n",
        "\n",
        "##################################\n",
        "####### Part 2: Training #########\n",
        "\n",
        "\n",
        "def train_network(config: AlphaZeroConfig, storage: SharedStorage,\n",
        "                  replay_buffer: ReplayBuffer):\n",
        "  network = Network()\n",
        "  optimizer = tf.train.MomentumOptimizer(config.learning_rate_schedule,\n",
        "                                         config.momentum)\n",
        "  for i in range(config.training_steps):\n",
        "    if i % config.checkpoint_interval == 0:\n",
        "      storage.save_network(i, network)\n",
        "    batch = replay_buffer.sample_batch()\n",
        "    update_weights(optimizer, network, batch, config.weight_decay)\n",
        "  storage.save_network(config.training_steps, network)\n",
        "\n",
        "\n",
        "def update_weights(optimizer, network: Network, batch,\n",
        "                   weight_decay: float):\n",
        "  loss = 0\n",
        "  for image, (target_value, target_policy) in batch:\n",
        "    value, policy_logits = network.inference(image)\n",
        "    loss += (\n",
        "        tf.losses.mean_squared_error(value, target_value) +\n",
        "        tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=policy_logits, labels=target_policy))\n",
        "\n",
        "  for weights in network.get_weights():\n",
        "    loss += weight_decay * tf.nn.l2_loss(weights)\n",
        "\n",
        "  optimizer.minimize(loss)\n",
        "\n",
        "\n",
        "######### End Training ###########\n",
        "##################################\n",
        "\n",
        "################################################################################\n",
        "############################# End of pseudocode ################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "# Stubs to make the typechecker happy, should not be included in pseudocode\n",
        "# for the paper.\n",
        "def softmax_sample(d):\n",
        "  return 0, 0\n",
        "\n",
        "\n",
        "def launch_job(f, *args):\n",
        "  f(*args)\n",
        "\n",
        "\n",
        "def make_uniform_network():\n",
        "  return Network()\n"
      ],
      "metadata": {
        "id": "WzfpqSEfxjj3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing\n"
      ],
      "metadata": {
        "id": "i19wFrjRR1F-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "game = Game()\n",
        "network = Network()\n",
        "root = Node(0)\n",
        "value, policy_logits = network.inference(game.make_image(-1))\n",
        "print(value)\n",
        "print(len(policy_logits))\n",
        "evaluate(root, game, network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "rj901PnKT1_Q",
        "outputId": "090ae805-8a89-493b-845f-5be4879e524c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 539ms/step\n",
            "[0.0224242]\n",
            "4672\n",
            "1/1 [==============================] - 0s 87ms/step\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-53d574ce032f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-13-ba9825c2707c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(node, game, network)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Expand the node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0mpolicy_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-ba9825c2707c>\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    102\u001b[0m   \u001b[0;31m# Expand the node.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m   \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_play\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m   \u001b[0mpolicy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy_logits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegal_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m   \u001b[0mpolicy_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitervalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miteritems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "myGame = Game()\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "myGame.apply(myGame.legal_actions()[0])\n",
        "print(len(myGame.history))\n",
        "index = -1\n",
        "for i in range(index,index-8,-1):\n",
        "  print(i)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EIMk41K-fZXp",
        "outputId": "ce882457-d7df-48f8-e5d9-e855246a1086"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "-1\n",
            "-2\n",
            "-3\n",
            "-4\n",
            "-5\n",
            "-6\n",
            "-7\n",
            "-8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create an optimizer with the desired parameters.\n",
        "opt = tf.compat.v1.train.MomentumOptimizer(learning_rate=0.05,momentum=0.9)\n",
        "var1 = 42\n",
        "# `loss` is a callable that takes no argument and returns the value\n",
        "# to minimize.\n",
        "loss = lambda: var1**2 - 5*var1 + 10\n",
        "\n",
        "# Initialize a list to store loss values\n",
        "loss_values = []\n",
        "\n",
        "# Call minimize to update the list of variables.\n",
        "for i in range(100):\n",
        "    opt.minimize(loss)\n",
        "    # Store the current loss value in the list\n",
        "    loss_values.append(loss().numpy())\n",
        "\n",
        "# Plot the loss values\n",
        "plt.plot(loss_values)\n",
        "plt.xlabel('Iteration')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs. Iteration')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "0ZBXw26tnPbk",
        "outputId": "7b4d65c4-fa26-47a5-dc46-90726bf32463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-74-7f8663957e01>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# Call minimize to update the list of variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;31m# Store the current loss value in the list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mloss_values\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mend_compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    472\u001b[0m     \"\"\"\n\u001b[0;32m--> 473\u001b[0;31m     grads_and_vars = self.compute_gradients(\n\u001b[0m\u001b[1;32m    474\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgate_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgate_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/training/optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[0;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[1;32m    564\u001b[0m       \u001b[0;31m# to be executed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1029\u001b[0m         flat_targets)\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mflat_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mbackprop_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIsTrainable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m         logging.vlog(\n\u001b[1;32m   1033\u001b[0m             \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"The dtype of the target tensor must be \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/backprop_util.py\u001b[0m in \u001b[0;36mIsTrainable\u001b[0;34m(tensor_or_dtype)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_or_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m   \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m   trainable_dtypes = [dtypes.float16, dtypes.float32, dtypes.float64,\n\u001b[1;32m     60\u001b[0m                       \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomplex128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/dtypes.py\u001b[0m in \u001b[0;36mas_dtype\u001b[0;34m(type_value)\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_INTERN_TABLE\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    828\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m   raise TypeError(f\"Cannot convert the argument `type_value`: {type_value!r} \"\n\u001b[0m\u001b[1;32m    830\u001b[0m                   \"to a TensorFlow DType.\")\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot convert the argument `type_value`: 1564 to a TensorFlow DType."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss_values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6LYgPjYtsAs",
        "outputId": "cb2e34da-a8a1-43f5-ddcc-071bc129a1de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1831.3125, 1484.0754, 1202.8137, 974.9916, 790.4557, 640.9817, 519.90765, 421.8377, 342.40103, 278.0573, 225.93889, 183.72299, 149.52812, 121.830284, 99.39502, 81.222466, 66.5027, 54.579693, 44.92205, 37.09936, 30.762985, 25.63052, 21.47322, 18.105808, 15.378206, 13.168848, 11.379267, 9.929707, 8.755562, 7.8045063, 7.034151, 6.41016, 5.90473, 5.495331, 5.163719, 4.895111, 4.67754, 4.5013094, 4.3585596, 4.2429333, 4.149276, 4.073414, 4.011965, 3.9621916, 3.921875, 3.8892183, 3.8627672, 3.841342, 3.8239865, 3.8099294, 3.798543, 3.7893195, 3.7818484, 3.7757978, 3.7708964, 3.7669253, 3.76371, 3.761105, 3.758995, 3.757286, 3.7559013, 3.7547808, 3.753872, 3.7531366, 3.75254, 3.752058, 3.7516665, 3.7513504, 3.7510934, 3.750886, 3.7507172, 3.7505808, 3.7504706, 3.7503815, 3.7503095, 3.7502499, 3.7502031, 3.7501645, 3.750133, 3.7501078, 3.7500868, 3.750071, 3.7500572, 3.7500463, 3.7500372, 3.7500305, 3.7500248, 3.7500205, 3.7500162, 3.7500134, 3.7500105, 3.750008, 3.7500072, 3.7500052, 3.7500052, 3.7500043, 3.7500029, 3.7500029, 3.750002, 3.7500014]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(var1, var2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8iEKw9C5r09R",
        "outputId": "0f25c269-ef9f-4ad9-c543-0d3a898c812d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.000104857536> <tf.Variable 'Variable:0' shape=() dtype=float32, numpy=0.012093236>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensor = numpy.zeros((8,8,73),dtype=numpy.float32)"
      ],
      "metadata": {
        "id": "Ht8bV1Nw2KPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "board = chess.Board()\n",
        "\n",
        "print(board.)"
      ],
      "metadata": {
        "id": "dbM8zr2tO3Eq"
      },
      "execution_count": 3,
      "outputs": []
    }
  ]
}